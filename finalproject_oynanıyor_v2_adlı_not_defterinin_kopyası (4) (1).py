# -*- coding: utf-8 -*-
"""FINALPROJECT oynanıyor v2 adlı not defterinin kopyası

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12Ssx9hK61zdeSiHVMpnRiqy0neHAfkDr

**Loading the data and libraries**
"""

#Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from google.colab import files

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
import xgboost as xgb
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import classification_report, accuracy_score, f1_score
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.linear_model import LogisticRegression
from mord import LogisticAT

import xgboost as xgb
import lightgbm as lgb
import catboost as ctb
import tensorflow as tf

# Configure settings for better output 0000000
warnings.filterwarnings('ignore')
pd.set_option('display.max_columns', None)
sns.set_style('whitegrid')

df = pd.read_csv('loans_full_schema.csv')

"""Summary Statistics!"""

# Converting object columns that are  categorical to the 'category' type xd
categorical_cols = [
    'state', 'homeownership', 'verified_income', 'verification_income_joint',
    'loan_purpose', 'application_type', 'grade', 'sub_grade',
    'loan_status', 'initial_listing_status', 'disbursement_method'
    ]
for col in categorical_cols:
    if col in df.columns:
        df[col] = df[col].astype('category')

#Summary Stats
print("Summary Statistics for NUMERICAL Columns")

numerical_summary = df.describe(include=np.number).round(2)
print(numerical_summary)
print("\n" + "="*80 + "\n")

print("Summary Statistics for CATEGORICAL Columns")
    # Using .describe() on categorical columns gives stats like count, unique values, top value, and its frequency.
categorical_summary = df.describe(include=['category', 'object'])
print(categorical_summary)

df['grade'] = df['grade'].astype(str)
grade_mapping = {
    'A': 'A',
    'B': 'B',
    'C': 'C',
    'D': 'D-G',
    'E': 'D-G',
    'F': 'D-G',
    'G': 'D-G'
}
df['grade'] = df['grade'].map(grade_mapping)

df['grade'] = pd.Categorical(df['grade'], categories=['A', 'B', 'C', 'D-G'])

print("Grade grouping complete. Final categories:", df['grade'].unique().tolist())

df.nunique().sort_values(ascending=False)

"""1. What is the overall distribution of loan grades?"""

emp_length_distribution = df['emp_length'].value_counts().sort_index()

print("Distribution of Employment Length ('emp_length'):")
print(emp_length_distribution)

debt_to_income = df['debt_to_income'].value_counts().sort_index()

print("Distribution of Employment Length ('debt_to_income'):")
print(debt_to_income)

months_since_last_credit_inquiry = df['months_since_last_credit_inquiry'].value_counts().sort_index()

print("Distribution of Employment Length ('months_since_last_credit_inquiry'):")
print(months_since_last_credit_inquiry)

sns.set_style("whitegrid")
plt.figure(figsize=(12, 7))

grade_counts = df['grade'].value_counts().sort_index()

ax = sns.barplot(x=grade_counts.index, y=grade_counts.values, palette="viridis")
ax.set_title('Distribution of Loan Grades', fontsize=16)
ax.set_xlabel('Loan Grade', fontsize=12)
ax.set_ylabel('Number of Loans', fontsize=12)

total_loans = len(df)
for p in ax.patches:
    count = int(p.get_height())
    percentage = f'{100 * count / total_loans:.1f}%'
    annotation = f'{count}\n({percentage})'
    x = p.get_x() + p.get_width() / 2
    y = p.get_height()
    ax.annotate(annotation, (x, y), ha='center', va='bottom', xytext=(0, 5), textcoords='offset points')

plt.tight_layout()
plt.show()

"""The distribution of loan grades is imbalanced. The majority of loans are concentrated in grades 'A', 'B', and 'C', which typically represent lower risk. Higher-risk grades ('D' through 'G') are progressively less common. Grade 'B' is the most frequent category, while 'G' is the rarest. This imbalance is critical! So afterwards we will apply some methods to solve it!

**3. Is there a connection between a borrower's annual income and the loan grade they receive?**
"""

sns.set_style("whitegrid")
plt.figure(figsize=(14, 8))

sns.boxplot(x='grade', y='annual_income', data=df[df['annual_income'] < 300000], order=sorted(df['grade'].unique()), palette="summer")
plt.title('Annual Income vs. Loan Grade (Outliers Removed)', fontsize=16)
plt.xlabel('Loan Grade', fontsize=12)
plt.ylabel('Annual Income ($)', fontsize=12)
plt.tight_layout()
plt.show()

"""Yes, there is a clear connection. Borrowers with higher annual incomes tend to receive better (lower-risk) loan grades. The median annual income for 'A' grade loans is visibly higher than for all subsequent grades. As the grade worsens, the median income generally decreases, suggesting that lenders view higher income as a strong indicator of creditworthiness. The presence of numerous outliers in all categories indicates that income alone is not the sole deciding factor.

4. How does homeownership status vary across different loan grades?
"""

grade_homeowner_ct = pd.crosstab(df['grade'], df['homeownership'], normalize='index') * 100

grade_homeowner_ct.plot(kind='bar', stacked=True, figsize=(14, 8), colormap='plasma')
plt.title('Homeownership Status by Loan Grade', fontsize=16)
plt.xlabel('Loan Grade', fontsize=12)
plt.ylabel('Percentage of Borrowers (%)', fontsize=12)
plt.xticks(rotation=0)
plt.legend(title='Homeownership')
plt.tight_layout()
plt.show()

"""The distribution of homeownership status changes across loan grades. Borrowers with mortgages make up the largest proportion of the higher-quality grades ('A', 'B', 'C'). For lower-quality grades ('D' and below), the proportion of borrowers who are renting increases significantly. This suggests that owning a home, and particularly having a mortgage, is associated with better loan grades, likely because it can be an indicator of financial stability.

What is the relationship between the loan's purpose and its assigned grade?
"""

df['grade'] = df['grade'].replace(['E', 'F', 'G'], 'E-G')

top_5_purposes = df['loan_purpose'].value_counts().nlargest(5).index

df_top_purposes = df[df['loan_purpose'].isin(top_5_purposes)]

plt.figure(figsize=(16, 8))
sns.countplot(data=df_top_purposes, x='grade', hue='loan_purpose', order=sorted(df_top_purposes['grade'].unique()), palette='magma')

plt.title('Loan Purpose Distribution Across Grades (Top 5 Purposes)', fontsize=16)
plt.xlabel('Loan Grade', fontsize=12)
plt.ylabel('Number of Loans', fontsize=12)
plt.legend(title='Loan Purpose')
plt.tight_layout()
plt.show()

"""The purpose of the loan appears to influence the grade distribution. Loans for 'credit card' refinancing and 'debt consolidation' are spread across all grades but make up a very large portion of the loans in riskier grades ('C', 'D', 'E-G'). In contrast, loans for 'major purchases' and 'home improvements' tend to be concentrated in the better grades. This indicates that lenders may perceive loans taken out to manage existing debt as inherently riskier than those for tangible assets or investments.

6. Does the length of a borrower's employment history correlate with their loan grade?
"""

emp_length_order = ['< 1 year', '1 year', '2 years', '3 years', '4 years', '5 years',
'6 years', '7 years', '8 years', '9 years', '10+ years']

df['emp_length_str'] = df['emp_length'].astype(str).str.replace('.0', '', regex=False) + ' years'
df['emp_length_str'] = df['emp_length_str'].replace({'0 years': '< 1 year', 'nan years': 'Unknown'})
df['emp_length_str'] = pd.Categorical(df['emp_length_str'], categories=emp_length_order, ordered=True)

emp_grade_ct = pd.crosstab(df['grade'], df['emp_length_str'], normalize='index') * 100

plt.figure(figsize=(15, 8))
sns.heatmap(emp_grade_ct, annot=True, fmt='.1f', cmap='YlGnBu')
plt.title('Employment Length Distribution (%) by Loan Grade', fontsize=16)
plt.xlabel('Employment Length', fontsize=12)
plt.ylabel('Loan Grade', fontsize=12)
plt.show()

"""There is a visible trend suggesting that longer employment history is associated with better loan grades. The proportion of borrowers with '10+ years' of employment is highest in grade 'A' and gradually decreases as the grade worsens. Conversely, the proportion of borrowers with shorter employment histories (e.g., '< 1 year' or '2 years') is higher in the riskier grades. This supports the hypothesis that lenders view longer employment as a sign of stability and lower risk."""

#7. How does the debt-to-income (DTI) ratio differ for borrowers across various loan grades?

sns.set_style("whitegrid")
plt.figure(figsize=(14, 8))

sns.boxplot(x='grade', y='debt_to_income', data=df[df['debt_to_income'] <= 60], order=sorted(df['grade'].unique()), palette="plasma")
plt.title('Debt-to-Income (DTI) Ratio vs. Loan Grade', fontsize=16)
plt.xlabel('Loan Grade', fontsize=12)
plt.ylabel('Debt-to-Income Ratio', fontsize=12)
plt.tight_layout()
plt.show()

"""There is a clear trend showing that lower DTI ratios are associated with better loan grades. The median DTI increases steadily as the loan grade moves from 'A' to 'G'. This is an intuitive finding, as a lower DTI indicates that a smaller portion of the borrower's income is already committed to debt payments, leaving more capacity to handle new loan installments. This suggests that DTI is a very important factor in the lender's risk assessment.

8. Is there a discernible pattern between a borrower's past delinquencies (delinq_2y) and the grade of their current loan?
This question investigates whether a history of late payments affects the perceived risk of a new loan.
"""

df['has_delinquency'] = df['delinq_2y'].apply(lambda x: '1+ Delinquencies' if x > 0 else '0 Delinquencies')

grade_delinq_ct = pd.crosstab(df['grade'], df['has_delinquency'], normalize='index') * 100

grade_delinq_ct.plot(kind='bar', stacked=True, figsize=(14, 8), colormap='coolwarm_r')
plt.title('Past Delinquency Status by Loan Grade', fontsize=16)
plt.xlabel('Loan Grade', fontsize=12)
plt.ylabel('Percentage of Borrowers (%)', fontsize=12)
plt.xticks(rotation=0)
plt.legend(title='Delinquency in Last 2 Years')
plt.tight_layout()
plt.show()

"""Yes, a clear pattern exists. The proportion of borrowers with zero delinquencies in the past two years is highest for 'A' grade loans and consistently decreases for riskier grades. Conversely, the percentage of borrowers with one or more delinquencies rises as the loan grade worsens. This demonstrates that a clean payment history is strongly associated with receiving a better loan grade, and past credit missteps are a significant factor in being assigned a higher-risk classification.

**Data Cleaning and Imputation**
"""

missing_values_count = df.isnull().sum()

variables_with_missing_values = missing_values_count[missing_values_count > 0].sort_values(ascending=False)

print("Variables with missing values:")
print(variables_with_missing_values)

num_variables_with_missing_values = len(variables_with_missing_values)
print(f"\nTotal number of variables with missing values: {num_variables_with_missing_values}")

import pandas as pd
import numpy as np
import missingno as msno
import matplotlib.pyplot as plt

cols_with_missing = [col for col in df.columns if df[col].isnull().any()]
df_missing_only = df[cols_with_missing]

print("\nOriginal data frame shape:", df.shape)
print("Just missing shape:", df_missing_only.shape)

msno.matrix(df_missing_only,sort='ascending', color=(0.5, 0.0, 0.5))
plt.show()

cols_with_missing = [col for col in df.columns if df[col].isnull().any()]

df_missing_only = df[cols_with_missing]

print("\nOriginal data frame shape:", df.shape)
print("Just missing:", df_missing_only.shape)

print("\nmsno.matrix() grafiği (sadece eksik verisi olan sütunlar için):")
msno.matrix(df_missing_only, color=(0.5, 0.0, 0.5))
plt.show()

# ------------------------------------------------------------------------------
#Part 2: Data Cleaning and Imputation
# ------------------------------------------------------------------------------
print("--- Data Cleaning and Imputation ---")
# Drop columns with over 30% missing values
missing_percentage = (df.isnull().sum() / len(df)) * 100
cols_to_drop_na = missing_percentage[missing_percentage > 30].index.tolist()

cols_danger = missing_percentage[missing_percentage > 10].index.tolist()


print("Aşağıdaki sütunlar %50'den fazla eksik veriye sahip olduğu için siliniyor:")
print(cols_to_drop_na)

df.drop(columns=cols_to_drop_na, inplace=True, errors='ignore')
print(f"Dropped {len(cols_to_drop_na)} columns with >30% missing values.")

print(f"of {len(cols_danger)} columns with >10% missing values.")
print(cols_danger)

df['emp_title'].fillna('Unknown', inplace=True)

"""['annual_income_joint', 'verification_income_joint', 'debt_to_income_joint', 'months_since_last_delinq', 'months_since_90d_late']
variables does have more than half
"""

from sklearn.experimental import enable_iterative_imputer # IterativeImputer'ı etkinleştirmek için
from sklearn.impute import IterativeImputer

# ------------------------------------------------------------------------------
# Part 2.1: Specific Imputation Strategies
# ------------------------------------------------------------------------------
print("--- Applying Specific Imputation Strategies ---")

mode_impute_cols = ['emp_length', 'num_accounts_120d_past_due']

print("\nMode  imputation")
for col in mode_impute_cols:
    if col in df.columns:
        mode_value = df[col].mode()[0]
        df[col].fillna(mode_value, inplace=True)
        print(f" - '{col}' filled with ({mode_value}) .")
    else:
        print(f" - '{col}' skip.")

# --- 2: Predictive Imputation (PMM-style) ---
pmm_impute_cols = ['debt_to_income', 'months_since_last_credit_inquiry']

pmm_impute_cols_exist = [col for col in pmm_impute_cols if col in df.columns]

if pmm_impute_cols_exist:
    print("\nPredictive Imputation (IterativeImputer) ile imputation yapılıyor...")

    imputer = IterativeImputer(max_iter=10, random_state=42)

    imputed_data = imputer.fit_transform(df[pmm_impute_cols_exist])

    df_imputed = pd.DataFrame(imputed_data, columns=pmm_impute_cols_exist, index=df.index)

    df[pmm_impute_cols_exist] = df_imputed

    for col in pmm_impute_cols_exist:
         print(f" - '{col}' sütunundaki eksik değerler diğer değişkenlere bakılarak dolduruldu.")
else:
    print("\nPredictive imputation için belirtilen sütunlar bulunamadı.")


# --- 3:Median ---
print("\nfilling with median...")
numerical_cols = df.select_dtypes(include=np.number).columns.tolist()
remaining_numerical_cols = [col for col in numerical_cols if col not in mode_impute_cols + pmm_impute_cols]

for col in remaining_numerical_cols:
    if df[col].isnull().any():
        median_value = df[col].median()
        df[col].fillna(median_value, inplace=True)
        print(f" - '{col}' filled with ({median_value}) ")

print("\ndone")
print("-" * 40 + "\n")

# ==============================================================================
# Part 3: Feature Engineering & Preprocessing (with Data Leakage Fix)
# ==============================================================================
print("--- Feature Engineering & Preprocessing (Corrected for Data Leakage) ---")
# Drop irrelevant columns and those that would leak information from the future
cols_to_drop_logic = [
    'emp_title', 'sub_grade', 'issue_month', 'loan_status',
    'paid_total', 'paid_principal', 'paid_interest', 'paid_late_fees', 'balance',
    'disbursement_method', 'initial_listing_status',
    'interest_rate', 'installment',
    'has_delinquency'
]

cols_to_drop_logic_filtered = [col for col in cols_to_drop_logic if col in df.columns]

df.drop(columns=cols_to_drop_logic_filtered, inplace=True, errors='ignore')

le = LabelEncoder()
df['grade'] = le.fit_transform(df['grade'])

df_encoded = pd.get_dummies(df, columns=df.select_dtypes(include=['object', 'category']).columns.tolist(), drop_first=True)


X = df_encoded.drop('grade', axis=1)
y = df_encoded['grade']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
print(f"Data split into training ({len(X_train)} rows) and testing ({len(X_test)} rows) sets.")

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
print("Data preprocessed and scaled.")
print("-" * 40 + "\n")

"""1. To Prevent Data Leakage (The "Cheating" Features)
These are variables whose values are determined by or after the loan grade is assigned. Including them would give the model the answer, leading to unrealistically high accuracy that wouldn't hold up with new data.

interest_rate & installment: The interest rate is a direct consequence of the loan's grade. A riskier grade gets a higher rate. The installment amount is calculated using this interest rate.

loan_status, paid_total, paid_principal, paid_interest, paid_late_fees, balance: All of these describe what happens after the loan has been issued. Our goal is to predict the grade at the moment of application, before any payments have been made or the loan status has had time to change.

2. To Remove Redundant or Directly Related Information
These variables are too similar to the target variable; grade.

sub_grade: This is just a more detailed version of grade (e.g., B1, B2, B3 are all within grade B)

3. For Practical Reasons (Irrelevant or Hard-to-Use Features)
These features are either not relevant to the borrower's creditworthiness or would require very complex processing to be useful.

emp_title: This column has thousands of unique job titles. Turning each one into a feature (one-hot encoding) would make analyse impossible. Not worth to add it.

issue_month: This relates to when the loan was issued, not the quality of the borrower. Irrelevent(Should I check?)

disbursement_method & initial_listing_status:  They are operational details, not characteristics of the borrower, and are unlikely to be useful predictors of grade.
"""

sns.set_style("whitegrid")

plt.figure(figsize=(12, 7))

grade_counts = df['grade'].value_counts().sort_index()

ax = sns.barplot(x=grade_counts.index, y=grade_counts.values, palette="viridis")

ax.set_title('Distribution of Loan Grades', fontsize=16)
ax.set_xlabel('Loan Grade', fontsize=12)
ax.set_ylabel('Number of Loans', fontsize=12)

total_loans = len(df)
for p in ax.patches:
    count = int(p.get_height())
    percentage = f'{100 * count / total_loans:.1f}%'
    annotation = f'{count}\n({percentage})'
    x = p.get_x() + p.get_width() / 2
    y = p.get_height()
    ax.annotate(annotation, (x, y), ha='center', va='bottom', xytext=(0, 5), textcoords='offset points', fontsize=11)

plt.tight_layout()

plt.show()

print("\nLoan Grade Counts and Percentages:")
grade_summary = pd.DataFrame({
    'Count': grade_counts,
    'Percentage': (grade_counts / total_loans) * 100
})
print(grade_summary.round(2))

# ==============================================================================
# Part 4: Model Training, Evaluation, and Comparison using GridSearchCV
# ==============================================================================
print("--- Model Training and Hyperparameter Tuning using GridSearchCV ---")

model_performance = []

print("Tuning Random Forest with GridSearchCV...")
rf_param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [10, 20],
    'min_samples_split': [5, 10],
    'min_samples_leaf': [2, 4]
}
rf_grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42, n_jobs=-1),
                              param_grid=rf_param_grid, cv=3, verbose=2, n_jobs=-1)
rf_grid_search.fit(X_train, y_train)
best_rf = rf_grid_search.best_estimator_
y_pred_rf = best_rf.predict(X_test)
rf_accuracy = accuracy_score(y_test, y_pred_rf)
rf_f1 = f1_score(y_test, y_pred_rf, average='weighted')
model_performance.append({'Model': 'Random Forest', 'Accuracy': rf_accuracy, 'Weighted F1-Score': rf_f1})
print("Random Forest tuning and evaluation complete.\n")
print(f"Best RF Params: {rf_grid_search.best_params_}\n")

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report

# --- 1. Generate and Visualize the Confusion Matrix ---
print("\n--- Generating Confusion Matrix ---")

try:
    class_names = le.classes_
except NameError:
    print("LabelEncoder 'le' not found. Inferring class names from y_test.")
    class_names = np.unique(y_test)

cm = confusion_matrix(y_test, y_pred_rf)
cm_df = pd.DataFrame(cm,
                     index=[str(c) for c in class_names],
                     columns=[str(c) for c in class_names])

plt.figure(figsize=(10, 8))
sns.heatmap(cm_df, annot=True, fmt='d', cmap='YlGnBu')
plt.title('Random Forest - Confusion Matrix', fontsize=16)
plt.ylabel('True Label', fontsize=12)
plt.xlabel('Predicted Label', fontsize=12)
plt.show()

print("\n--- Detailed Classification Report ---")
report = classification_report(y_test, y_pred_rf, target_names=[str(c) for c in class_names])
print(report)
print("-" * 40)


# --- 2. Generate and Visualize Feature Importances ---

print("\n--- Generating Feature Importance Plot ---")

feature_importances = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': best_rf.feature_importances_
})

top_20_features = feature_importances.sort_values(by='Importance', ascending=False).head(20)

plt.figure(figsize=(12, 10))
sns.barplot(x='Importance', y='Feature', data=top_20_features, palette='viridis')
plt.title('Top 20 Feature Importances from Random Forest', fontsize=16)
plt.xlabel('Importance Score', fontsize=12)
plt.ylabel('Feature', fontsize=12)
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

# --- Model 2: Support Vector Machine (Tuned) ---
print("Training Tuned SVM... (This may take a moment)")
svm_tuned = SVC(C=10, gamma=0.001, kernel='rbf', random_state=42, probability=True) # probability=True is needed for some plots
svm_tuned.fit(X_train_scaled, y_train)
y_pred_svm = svm_tuned.predict(X_test_scaled)
svm_accuracy = accuracy_score(y_test, y_pred_svm)
svm_f1 = f1_score(y_test, y_pred_svm, average='weighted')
model_performance.append({'Model': 'SVM', 'Accuracy': svm_accuracy, 'Weighted F1-Score': svm_f1})
print("SVM evaluation complete.\n")

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# --- SVM Model Evaluation Details ---
print("--- SVM Classification Report ---")
report = classification_report(y_test, y_pred_svm)
print(report)


print("\n--- SVM Confusion Matrix ---")
cm = confusion_matrix(y_test, y_pred_svm)

if hasattr(y_test, 'unique'):
    class_names = sorted(y_test.unique())
else:
    class_names = sorted(np.unique(y_test))

plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_names, yticklabels=class_names)

plt.title('SVM - Confusion Matrix', fontsize=16)
plt.ylabel('True Label', fontsize=12)
plt.xlabel('Predicted Label', fontsize=12)
plt.show()

# --- Model 3: XGBoost (Tuned) ---
print("Training Tuned XGBoost...")
xgb_tuned = xgb.XGBClassifier(objective='multi:softmax', num_class=len(le.classes_), use_label_encoder=False, eval_metric='mlogloss', subsample=0.7, n_estimators=300, max_depth=7, learning_rate=0.01, colsample_bytree=0.7, random_state=42, n_jobs=-1)
xgb_tuned.fit(X_train, y_train)
y_pred_xgb = xgb_tuned.predict(X_test)
xgb_accuracy = accuracy_score(y_test, y_pred_xgb)
xgb_f1 = f1_score(y_test, y_pred_xgb, average='weighted')
model_performance.append({'Model': 'XGBoost', 'Accuracy': xgb_accuracy, 'Weighted F1-Score': xgb_f1})
print("XGBoost evaluation complete.\n")

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix, cohen_kappa_score, accuracy_score

accuracy = accuracy_score(y_test, y_pred_xgb)
print(f"--- Overall Model Performance ---")
print(f"Accuracy: {accuracy:.4f}")

kappa = cohen_kappa_score(y_test, y_pred_xgb)
print(f"Cohen's Kappa Statistic: {kappa:.4f}")
print("-" * 40)

try:
    class_names = le.classes_
except NameError:
    print("LabelEncoder 'le' not found. Inferring class names from y_test.")
    class_names = np.unique(y_test)

print("\n--- Detailed Classification Report ---")
report = classification_report(y_test, y_pred_xgb, target_names=[str(c) for c in class_names])
print(report)
print("-" * 40)

print("\n--- Confusion Matrix ---")
cm = confusion_matrix(y_test, y_pred_xgb)
cm_df = pd.DataFrame(cm,
                     index = [str(c) for c in class_names],
                     columns = [str(c) for c in class_names])

# Plotting the confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')
plt.title('XGBoost - Confusion Matrix', fontsize=16)
plt.ylabel('True Label', fontsize=12)
plt.xlabel('Predicted Label', fontsize=12)
plt.show()

# --- Model 4: Artificial Neural Network (ANN) ---
print("Training ANN...")
ann_model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),
    Dropout(0.5),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(len(le.classes_), activation='softmax')
])
ann_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
ann_model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.1, callbacks=[early_stopping], verbose=0)
y_pred_proba_ann = ann_model.predict(X_test_scaled)
y_pred_ann = np.argmax(y_pred_proba_ann, axis=1)
ann_accuracy = accuracy_score(y_test, y_pred_ann)
ann_f1 = f1_score(y_test, y_pred_ann, average='weighted')
model_performance.append({'Model': 'ANN', 'Accuracy': ann_accuracy, 'Weighted F1-Score': ann_f1})
print("ANN evaluation complete.\n")
print("-" * 40 + "\n")

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, f1_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model

# --- Model 4: Artificial Neural Network (ANN) ---
print("Training ANN...")
ann_model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),
    Dropout(0.5),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(len(le.classes_), activation='softmax')
])
ann_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

history = ann_model.fit(X_train_scaled, y_train,
                        epochs=100,
                        batch_size=32,
                        validation_split=0.1,
                        callbacks=[early_stopping],
                        verbose=0) # verbose=0 hides the epoch-by-epoch output for a cleaner run

y_pred_proba_ann = ann_model.predict(X_test_scaled)
y_pred_ann = np.argmax(y_pred_proba_ann, axis=1)
ann_accuracy = accuracy_score(y_test, y_pred_ann)
ann_f1 = f1_score(y_test, y_pred_ann, average='weighted')
print("ANN evaluation complete.\n")
print("-" * 40 + "\n")


# --- VISUALIZATION 1: MODEL ARCHITECTURE PLOT ---

print("--- Generating Model Architecture Plot ---")
plot_model(ann_model, to_file='ann_model_plot2.png', show_shapes=True, show_layer_names=True, rankdir='TB')
print("Model architecture plot saved as 'ann_model_plot.png'")

!pip install pydot
!pip install graphviz

print("\n--- Generating Training History Plots ---")

fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 12))

# Plot training & validation accuracy values
ax1.plot(history.history['accuracy'])
ax1.plot(history.history['val_accuracy'])
ax1.set_title('Model Accuracy')
ax1.set_ylabel('Accuracy')
ax1.set_xlabel('Epoch')
ax1.legend(['Train', 'Validation'], loc='upper left')
ax1.grid(True)

# Plot training & validation loss values
ax2.plot(history.history['loss'])
ax2.plot(history.history['val_loss'])
ax2.set_title('Model Loss')
ax2.set_ylabel('Loss')
ax2.set_xlabel('Epoch')
ax2.legend(['Train', 'Validation'], loc='upper left')
ax2.grid(True)

plt.tight_layout()
plt.show()

# ---  5: GridSearchCV with LightGBM ---
print("GridSearchCV ile LightGBM optimize ediliyor...")
lgbm_param_grid = {'n_estimators': [100, 200], 'learning_rate': [0.05, 0.1], 'max_depth': [5, 7], 'num_leaves': [20, 31]}
lgbm_grid_search = GridSearchCV(estimator=lgb.LGBMClassifier(objective='multiclass', random_state=42, n_jobs=-1), param_grid=lgbm_param_grid, cv=3, verbose=2, n_jobs=-1)
lgbm_grid_search.fit(X_train, y_train)
best_lgbm = lgbm_grid_search.best_estimator_
print("\nBulunan En İyi LightGBM Parametreleri:", lgbm_grid_search.best_params_)
y_pred_lgbm = best_lgbm.predict(X_test)
lgbm_accuracy = accuracy_score(y_test, y_pred_lgbm)
lgbm_f1 = f1_score(y_test, y_pred_lgbm, average='weighted')
model_performance.append({'Model': 'LightGBM', 'Accuracy': lgbm_accuracy, 'Weighted F1-Score': lgbm_f1})
print("LightGBM değerlendirmesi tamamlandı.\n")

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report

print("\n--- Generating Confusion Matrix ---")

try:
    class_names = le.classes_
except NameError:
    print("LabelEncoder 'le' not found. Inferring class names from y_test.")
    class_names = np.unique(y_test)

cm = confusion_matrix(y_test, y_pred_lgbm)
cm_df = pd.DataFrame(cm,
                     index = [str(c) for c in class_names],
                     columns = [str(c) for c in class_names])

plt.figure(figsize=(10, 8))
sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')
plt.title('LightGBM - Confusion Matrix', fontsize=16)
plt.ylabel('True Label', fontsize=12)
plt.xlabel('Predicted Label', fontsize=12)
plt.show()

print("\n--- Detailed Classification Report ---")
report = classification_report(y_test, y_pred_lgbm, target_names=[str(c) for c in class_names])
print(report)
print("-" * 40)


print("\n--- Generating Feature Importance Plot ---")

feature_importances = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': best_lgbm.feature_importances_
})

top_12_features = feature_importances.sort_values(by='Importance', ascending=False).head(12)

plt.figure(figsize=(12, 10))
sns.barplot(x='Importance', y='Feature', data=top_12_features, palette='viridis')
plt.title('Top 12 Feature Importances from LightGBM', fontsize=16)
plt.xlabel('Importance Score', fontsize=12)
plt.ylabel('Feature', fontsize=12)
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform, randint
import catboost as ctb
from sklearn.metrics import accuracy_score, f1_score

catb_param_dist = {
    'iterations': randint(200, 600),
    'learning_rate': uniform(0.03, 0.2),
    'depth': randint(6, 11),
    'l2_leaf_reg': randint(1, 10),
    'subsample': uniform(0.7, 0.3)
}


catb_random_search = RandomizedSearchCV(
    estimator=ctb.CatBoostClassifier(loss_function='MultiClass', random_state=42, verbose=0, bootstrap_type='Bernoulli'),
    param_distributions=catb_param_dist,
    n_iter=30,
    cv=4,
    verbose=2,
    n_jobs=-1,
    random_state=42
)


print("RandomizedSearchCV ile CatBoost optimize ediliyor...")
catb_random_search.fit(X_train, y_train)

print("\nBulunan En İyi CatBoost Parametreleri:", catb_random_search.best_params_)

best_catb_final = ctb.CatBoostClassifier(
    **catb_random_search.best_params_,
    loss_function='MultiClass',
    random_state=42,
    verbose=100,

    bootstrap_type='Bernoulli'
)



best_catb_final.fit(X_train, y_train)

y_pred_catb = best_catb_final.predict(X_test)
catb_accuracy = accuracy_score(y_test, y_pred_catb)
catb_f1 = f1_score(y_test, y_pred_catb, average='weighted')

model_performance.append({'Model': 'CatBoost (Randomized)', 'Accuracy': catb_accuracy, 'Weighted F1-Score': catb_f1})
print("CatBoost değerlendirmesi tamamlandı.\n")

#Feature Importance

print("--- CatBoost Feature Importance ---")

feature_importances = best_catb_final.get_feature_importance()
feature_names = X_train.columns

importance_df = pd.DataFrame({
    'feature': feature_names,
    'importance': feature_importances
})

importance_df = importance_df.sort_values(by='importance', ascending=False)

print("Top20 Feature:")
print(importance_df.head(20))

plt.figure(figsize=(12, 10))
sns.barplot(
    x='importance',
    y='feature',
    data=importance_df.head(20),
    palette='viridis'
)

plt.title('CatBoost -Top20', fontsize=16)
plt.xlabel('Importance Score', fontsize=12)
plt.ylabel('Feature Name', fontsize=12)
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score
import catboost as ctb


print("--- CatBoost Feature Importance  ---")

feature_importances = best_catb_final.get_feature_importance()
feature_names = X_train.columns

importance_df = pd.DataFrame({
    'feature': feature_names,
    'importance': feature_importances
})
importance_df = importance_df.sort_values(by='importance', ascending=False)

plt.figure(figsize=(12, 10))
sns.barplot(
    x='importance',
    y='feature',
    data=importance_df.head(20),
    palette='viridis'
)
plt.title('CatBoost - Top 20 ', fontsize=16)
plt.xlabel('Importance Score', fontsize=12)
plt.ylabel('Feature Name', fontsize=12)
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()



try:
    class_names = le.classes_
except NameError:
    print("LabelEncoder 'le' not found")
    class_names = np.unique(y_test)


report = classification_report(y_test, y_pred_catb, target_names=[str(c) for c in class_names])
print(report)
print("-" * 40)


# -Confusion Matrix
print("\n--- CatBoost CM  ---")

cm = confusion_matrix(y_test, y_pred_catb)
cm_df = pd.DataFrame(cm,
                     index=[str(c) for c in class_names],
                     columns=[str(c) for c in class_names])

plt.figure(figsize=(10, 8))
sns.heatmap(cm_df, annot=True, fmt='d', cmap='Greens')
plt.title('CatBoost - CM', fontsize=16)
plt.ylabel('True Label', fontsize=12)
plt.xlabel('TPredicted Label', fontsize=12)
plt.show()

# --- Model 7: Multinomial Logistic Regression ---
print("GridSearchCV with Multinomial Logistic Regression ...")
log_reg_param_grid = {'C': [0.1, 1.0, 10], 'solver': ['lbfgs'], 'multi_class': ['multinomial']}
log_reg_grid_search = GridSearchCV(estimator=LogisticRegression(random_state=42, max_iter=1000), param_grid=log_reg_param_grid, cv=3, verbose=2, n_jobs=-1)
log_reg_grid_search.fit(X_train_scaled, y_train)
best_log_reg = log_reg_grid_search.best_estimator_
print("\nBest Multinomial Logistic Regression Parametreleri:", log_reg_grid_search.best_params_)
y_pred_log_reg = best_log_reg.predict(X_test_scaled)
log_reg_accuracy = accuracy_score(y_test, y_pred_log_reg)
log_reg_f1 = f1_score(y_test, y_pred_log_reg, average='weighted')
model_performance.append({'Model': 'Multinomial Regression', 'Accuracy': log_reg_accuracy, 'Weighted F1-Score': log_reg_f1})
print("Multinomial Lojistik Regresyon değerlendirmesi tamamlandı.\n")

# --- YENİ Model 8: Ordinal Logistic Regression ---
print("Ordinal Lojistik Regresyon...")
ord_reg = LogisticAT(alpha=1.0)
ord_reg.fit(X_train_scaled, y_train)
y_pred_ord_reg = ord_reg.predict(X_test_scaled)
ord_reg_accuracy = accuracy_score(y_test, y_pred_ord_reg)
ord_reg_f1 = f1_score(y_test, y_pred_ord_reg, average='weighted')
model_performance.append({'Model': 'Ordinal Regression', 'Accuracy': ord_reg_accuracy, 'Weighted F1-Score': ord_reg_f1})
print("Ordinal Lojistik Regresyon değerlendirmesi tamamlandı.\n")

# --- Model 7: Multinomial Logistic Regression ---
print("GridSearchCV ile Multinomial Lojistik Regresyon optimize ediliyor...")
log_reg_param_grid = {'C': [0.1, 1.0, 10], 'solver': ['lbfgs'], 'multi_class': ['multinomial']}
log_reg_grid_search = GridSearchCV(estimator=LogisticRegression(random_state=42, max_iter=1000), param_grid=log_reg_param_grid, cv=3, verbose=2, n_jobs=-1)
log_reg_grid_search.fit(X_train_scaled, y_train)
best_log_reg = log_reg_grid_search.best_estimator_
print("\nBulunan En İyi Multinomial Lojistik Regresyon Parametreleri:", log_reg_grid_search.best_params_)
y_pred_log_reg = best_log_reg.predict(X_test_scaled)
log_reg_accuracy = accuracy_score(y_test, y_pred_log_reg)
log_reg_f1 = f1_score(y_test, y_pred_log_reg, average='weighted')
model_performance.append({'Model': 'Multinomial Regression', 'Accuracy': log_reg_accuracy, 'Weighted F1-Score': log_reg_f1})
print("Multinomial Lojistik Regresyon değerlendirmesi tamamlandı.\n")


# --- YENİ Model 8: Ordinal Logistic Regression (FIXED) ---
print("Ordinal Lojistik Regresyon eğitiliyor...")


ord_reg = LogisticAT(alpha=0.01)

ord_reg.fit(X_train_scaled, y_train)
y_pred_ord_reg = ord_reg.predict(X_test_scaled)
ord_reg_accuracy = accuracy_score(y_test, y_pred_ord_reg)
ord_reg_f1 = f1_score(y_test, y_pred_ord_reg, average='weighted')
model_performance.append({'Model': 'Ordinal Regression', 'Accuracy': ord_reg_accuracy, 'Weighted F1-Score': ord_reg_f1})
print("Ordinal Lojistik Regresyon değerlendirmesi tamamlandı.\n")

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix

try:
    class_names = le.classes_
except NameError:
    print("LabelEncoder 'le' not found. Inferring class names from y_test.")
    class_names = np.unique(y_test)


# --- 1. Multinomial Logistic Regression Analysis ---
print("--- Multinomial Logistic Regression Performance ---")

# Generate and print the classification report
print("\n--- Detailed Classification Report ---")
report_multi = classification_report(y_test, y_pred_log_reg, target_names=[str(c) for c in class_names])
print(report_multi)

# Generate and visualize the confusion matrix
print("\n--- Confusion Matrix ---")
cm_multi = confusion_matrix(y_test, y_pred_log_reg)
cm_df_multi = pd.DataFrame(cm_multi,
                           index=[str(c) for c in class_names],
                           columns=[str(c) for c in class_names])

plt.figure(figsize=(10, 8))
sns.heatmap(cm_df_multi, annot=True, fmt='d', cmap='Oranges')
plt.title('Multinomial Logistic Regression - Confusion Matrix', fontsize=16)
plt.ylabel('True Label', fontsize=12)
plt.xlabel('Predicted Label', fontsize=12)
plt.show()

print("-" * 60)


# --- 2. Ordinal Logistic Regression Analysis ---
print("\n--- Ordinal Logistic Regression Performance ---")

# Generate and print the classification report
print("\n--- Detailed Classification Report ---")
report_ord = classification_report(y_test, y_pred_ord_reg, target_names=[str(c) for c in class_names])
print(report_ord)

# Generate and visualize the confusion matrix
print("\n--- Confusion Matrix ---")
cm_ord = confusion_matrix(y_test, y_pred_ord_reg)
cm_df_ord = pd.DataFrame(cm_ord,
                         index=[str(c) for c in class_names],
                         columns=[str(c) for c in class_names])

plt.figure(figsize=(10, 8))
sns.heatmap(cm_df_ord, annot=True, fmt='d', cmap='Greens')
plt.title('Ordinal Logistic Regression - Confusion Matrix', fontsize=16)
plt.ylabel('True Label', fontsize=12)
plt.xlabel('Predicted Label', fontsize=12)
plt.show()

# ==============================================================================
# Part 5: Final Model Comparison
# ==============================================================================
print("--- Final Model Comparison ---")

# Create a DataFrame from the results
performance_df = pd.DataFrame(model_performance)
performance_df.set_index('Model', inplace=True)
performance_df = performance_df.sort_values(by='Accuracy', ascending=False)

print("Performance Summary Table:")
print(performance_df)
print("\n")

# Visualize the results
plt.figure(figsize=(12, 7))
ax = sns.barplot(x=performance_df.index, y=performance_df['Accuracy'], palette="viridis")
plt.title('Model Comparison: Test Set Accuracy', fontsize=16)
plt.ylabel('Accuracy Score', fontsize=12)
plt.xlabel('Model', fontsize=12)
plt.xticks(rotation=0)
plt.ylim(0.0, max(performance_df['Accuracy']) * 1.2) # Dynamic y-limit

# Add accuracy values on top of the bars
for p in ax.patches:
    ax.annotate(f"{p.get_height():.4f}",
                (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center',
                xytext=(0, 9),
                textcoords='offset points',
                fontsize=12)
plt.tight_layout()
plt.show()

# ==============================================================================
# Bölüm 8: Likelihood Ratio Test - LRT
# ==============================================================================
print("--- Olabilirlik Oran Testi Başlatılıyor ---")

from sklearn.metrics import log_loss
from scipy.stats import chi2
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
import pandas as pd


if 'best_rf' in locals() and 'X_train' in locals():
    importances = best_rf.feature_importances_
    feature_names = X_train.columns
    feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

    reduced_feature_names = feature_importance_df.head(15)['Feature'].tolist()

    X_train_reduced = X_train[reduced_feature_names]
    X_test_reduced = X_test[reduced_feature_names]

    scaler_reduced = StandardScaler()
    X_train_reduced_scaled = scaler_reduced.fit_transform(X_train_reduced)
    X_test_reduced_scaled = scaler_reduced.transform(X_test_reduced)

    print(f"İndirgenmiş model için {len(reduced_feature_names)} özellik seçildi.")


    full_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', C=1.0, max_iter=1000, random_state=42)
    full_model.fit(X_train_scaled, y_train)
    print("done.")

    reduced_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', C=1.0, max_iter=1000, random_state=42)
    reduced_model.fit(X_train_reduced_scaled, y_train)
    print("done.")


    ll_full = -log_loss(y_test, full_model.predict_proba(X_test_scaled), normalize=False)
    ll_reduced = -log_loss(y_test, reduced_model.predict_proba(X_test_reduced_scaled), normalize=False)

    print(f"\nFull Model Log-Likelihood: {ll_full:.4f}")
    print(f"Red Model Log-Likelihood: {ll_reduced:.4f}")

    LR_statistic = 2 * (ll_full - ll_reduced)
    print(f"\nLikelihood Ratio (LR) : {LR_statistic:.4f}")

    degrees_of_freedom = X_train_scaled.shape[1] - X_train_reduced_scaled.shape[1]
    print(f"df: {degrees_of_freedom}")


    p_value = chi2.sf(LR_statistic, degrees_of_freedom)
    print(f"P-val: {p_value:.4f}")